{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "working with data techniques:   sentiment analysis | topic modeling | text generation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PROGRAMMING -\n",
    "    data analysis = Pandas, sklearn, re\n",
    "    NLP = nltk, TextBlob, gensim\n",
    "\n",
    "MATH -\n",
    "    Clean data = corpus, document-term matrix\n",
    "    Exploratory Data Analysis = word counts\n",
    "    NLP = sentiment analysis, topic modeling, text generation\n",
    "    \n",
    "COMMUNICATION -\n",
    "    design = scope, visualize, extract insights\n",
    "    domain = expertise"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* * * * * * * DATA SCIENCE WORKFLOW * * * * * * * * * * \n",
    "\n",
    "1. start with question\n",
    "2. get & clean the data\n",
    "3. do EDA, make scatterplot, any corelations?\n",
    "4. apply technique, apply linear regression\n",
    "5. share insights, sum up study\n",
    "\n",
    "* * * * * * * * * * * * * * * * * * * * * * * * * * * * "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#1. how to get data?      = Transcripts. \n",
    "    How much data to get? = 10    \n",
    "    Time range?           = 5 yrs\n",
    "\n",
    "WEB SCRAPING \n",
    " - requests  HTTP or from website\n",
    " - BeautifulSoup  - parse HTML or extract from website\n",
    "\n",
    "Save Data = Pickle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#2. Data cleaning and have in standard format\n",
    "    \n",
    "    corpus = collection of texts              << use Pandas library DataFrame for a table\n",
    "        \n",
    "    document-term matrix =\n",
    "        1. clean text - remove excess parts of text\n",
    "        2. tokenize text - splot text into small parts\n",
    "        3.DTM - put into matrix so machine can read it\n",
    "\n",
    "clean text:\n",
    "    - remove punctuation\n",
    "    - lowercase letters\n",
    "    - remove numbers                         << use regular expression (search for patterns)\n",
    "    \n",
    "tokenize by word, each word is on its own \n",
    "filter out STOP words => \"bag of words model\"\n",
    "\n",
    "put into matrix DTM \n",
    "    - each row is different document\n",
    "    - each column is a different term\n",
    "    - values are word counts                << use scikit-learn & count vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# scrape from the website using a function\n",
    "def url_to_transcript(url):\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = [p.text for p in soup.find(class=\"{whatever website labels the text}\").findall('p')]\n",
    "    print(url)\n",
    "    return text\n",
    "\n",
    "# URLS of transcripts \n",
    "urls = [ ]\n",
    "\n",
    "# label names\n",
    "names = [ ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# make new directory to hold text files:    !mkdir transcripts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pickle files for later use\n",
    "\n",
    "for i, c in enumerate(names):\n",
    "    with open('transcripts/', + c + \".txt\") as file:\n",
    "        pickle.dump(transcripts[i], file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load pickled files\n",
    "\n",
    "data = {}\n",
    "\n",
    "for i, c in enumerate(names):\n",
    "    with open('transcripts/', + c + \".txt\") as file:\n",
    "        data[c] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check to see if data is loaded correctly\n",
    "data.keys()\n",
    "\n",
    "data['name1'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to combine all text strings\n",
    "\n",
    "def combine_text(list_of_text):\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# combine it\n",
    "\n",
    "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# can change data into a dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "data_df.columns = ['transcript']\n",
    "data_df = data_df.sort_index()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)                               # drop punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  # drop all items inside []\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)                             # drop alphanums and digits\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_clean = pd.DataFrame(data_df.transcript.apply(round1))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_clean = pd.DataFrame(data_clean.transcript.apply(round2))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add full names\n",
    "# full_name = []\n",
    "# data_df['full_name'] = full_name\n",
    "\n",
    "#data_df.to_pickle('corpus.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# document term matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(data_clean.transcript)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "steps\n",
    "1. data - determine the format of the raw data needed to start\n",
    "2. aggregate - figure out how to aggregate the data\n",
    "3. visualize the data\n",
    "4. extract key findings from visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
